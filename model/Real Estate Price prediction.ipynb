{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a491c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (20,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617bde1a",
   "metadata": {},
   "source": [
    "### __Data Load: Load banglore home prices into a dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e26bcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\guptdraj\\\\OneDrive - TietoEVRY\\\\Desktop\\\\Python notebook\\\\Bengaluru_House_Data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12936/144017554.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\guptdraj\\OneDrive - TietoEVRY\\Desktop\\Python notebook\\Bengaluru_House_Data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\guptdraj\\\\OneDrive - TietoEVRY\\\\Desktop\\\\Python notebook\\\\Bengaluru_House_Data.csv'"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(r\"C:\\Users\\guptdraj\\OneDrive - TietoEVRY\\Desktop\\Python notebook\\Bengaluru_House_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740379d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4828bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('area_type')['area_type'].agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d341794",
   "metadata": {},
   "source": [
    "__Drop features that are not required to build our model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529200a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data.drop(['area_type','availability','society','balcony'],axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e655de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd032af",
   "metadata": {},
   "source": [
    "__DATA CLEANING PROCESS__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f83d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08c0d0e",
   "metadata": {},
   "source": [
    "Since the dataset has large number of rows and null values are very less we can drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b847967",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data1.dropna()\n",
    "data2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6442d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd54b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['size'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d76209",
   "metadata": {},
   "source": [
    "### __Feature Engineering__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11937e61",
   "metadata": {},
   "source": [
    "__Add new feature(integer) for bhk (Bedrooms Hall Kitchen)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5977f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['bhk'] = data2['size'].apply(lambda x: int(x.split(' ')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a19762",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2['bhk'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f2325",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[data2.bhk>20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5c8ae2",
   "metadata": {},
   "source": [
    "__Explore total_sqft feature__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd048e93",
   "metadata": {},
   "source": [
    "data2.total_sqft.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080c8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_float(x):\n",
    "    try:\n",
    "        float(x)\n",
    "    except:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9914d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2[~data2['total_sqft'].apply(is_float)].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a94134",
   "metadata": {},
   "source": [
    "__Above shows that total_sqft can be a range (e.g. 2100-2850). For such case we can just take average of min and max value in the range. There are other cases such as 34.46Sq. Meter which one can convert to square ft using unit conversion. I am going to just drop such corner cases to keep things simple__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0908ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sqft_to_num(x):\n",
    "    tokens=x.split('-')\n",
    "    if len(tokens) == 2:\n",
    "        return (float(tokens[0])+float(tokens[1]))/2\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673aaec",
   "metadata": {},
   "source": [
    "__For below row, it shows total_sqft as 2475 which is an average of the range 2100-2850__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_sqft_to_num('2100 - 2850')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d015afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data2.copy()\n",
    "data3['total_sqft'] = data3['total_sqft'].apply(convert_sqft_to_num)\n",
    "data3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37968ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.loc[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c26cbee",
   "metadata": {},
   "source": [
    "__FEATURE ENGINEERING__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a95647",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96beafb",
   "metadata": {},
   "source": [
    "__Add new feature called price per square feet__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = data3.copy()\n",
    "data4['price_per_sqft'] = data4['price']*100000/data4['total_sqft']\n",
    "data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d229ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data4['location'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc508ce",
   "metadata": {},
   "source": [
    "__Examine locations which is a categorical variable. We need to apply dimensionality reduction technique here to reduce number of locations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e9e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4.location = data4.location.apply(lambda x:x.strip())\n",
    "\n",
    "location_stats = data4.groupby('location')['location'].agg('count').sort_values(ascending = False)\n",
    "location_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfda2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(location_stats[location_stats<=10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d96817",
   "metadata": {},
   "source": [
    "### __Dimensionality Reduction__\n",
    "\n",
    "__Any location having less than 10 data points should be tagged as \"other\" location. This way number of categories can be reduced by huge amount. Later on when we do one hot encoding, it will help us with having fewer dummy columns__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb705d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_stats_less_than_10 = location_stats[location_stats<=10]\n",
    "location_stats_less_than_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14667088",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4.location = data4.location.apply(lambda x: 'other'if x in location_stats_less_than_10 else x)\n",
    "len(data4.location.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d906d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2595cd4b",
   "metadata": {},
   "source": [
    "### __Outlier Removal Using Business Logic__\n",
    "\n",
    "__As a data scientist when you have a conversation with your business manager (who has expertise in real estate), he will tell you that normally square ft per bedroom is 300 (i.e. 2 bhk apartment is minimum 600 sqft. If you have for example 400 sqft apartment with 2 bhk than that seems suspicious and can be removed as an outlier. We will remove such outliers by keeping our minimum thresold per bhk to be 300 sqft__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9669c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data4[data4.total_sqft/data4.bhk<300].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec21c383",
   "metadata": {},
   "source": [
    "__Check above data points. We have 6 bhk apartment with 1020 sqft. Another one is 8 bhk and total sqft is 600. These are clear data errors that can be removed safely__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843d1534",
   "metadata": {},
   "outputs": [],
   "source": [
    "data5 = data4[~(data4.total_sqft/data4.bhk<300)]\n",
    "data5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842155cc",
   "metadata": {},
   "source": [
    "### __Outlier Removal Using Standard Deviation and Mean__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "data5.price_per_sqft.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3320a74",
   "metadata": {},
   "source": [
    "__Here we find that min price per sqft is 267 rs/sqft whereas max is 176470, this shows a wide variation in property prices. We should remove outliers per location using mean and one standard deviation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502d92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pps_outliers(df):\n",
    "    \n",
    "    df_out=pd.DataFrame()\n",
    "    for key, subdf in df.groupby ('location'):\n",
    "        m = np.mean(subdf.price_per_sqft)\n",
    "        st = np.std(subdf.price_per_sqft)\n",
    "        reduced_df = subdf[(subdf.price_per_sqft> (m-st))&(subdf.price_per_sqft<=(m+st))]\n",
    "        df_out = pd.concat([df_out, reduced_df],ignore_index=True)\n",
    "    return df_out\n",
    "\n",
    "data6=remove_pps_outliers(data5)\n",
    "data6.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef8d4d7",
   "metadata": {},
   "source": [
    "__Let's check if for a given location how does the 2 BHK and 3 BHK property prices look like__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014b807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_chart(df, location):\n",
    "    bhk2=df[(df.location==location)&(df.bhk==2)]\n",
    "    bhk3=df[(df.location==location)&(df. bhk==3)]\n",
    "    matplotlib.rcParams['figure.figsize']=(15,18)\n",
    "    plt.scatter(bhk2.total_sqft, bhk2.price,color='blue',label='2 BHK', s=50)\n",
    "    plt.scatter(bhk3. total_sqft, bhk3.price, marker ='+', color='green',label='3 BHK', s=50)\n",
    "    plt.xlabel(\"Total Square Feet Area\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.title(location)\n",
    "    plt.legend()\n",
    "                            \n",
    "plot_scatter_chart(data6, \"Rajaji Nagar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c08428",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_chart(data6, \"Hebbal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881d31af",
   "metadata": {},
   "source": [
    "__We should also remove properties where for same location, the price of (for example) 3 bedroom apartment is less than 2 bedroom apartment (with same square ft area). What we will do is for a given location, we will build a dictionary of stats per bhk, i.e.__\n",
    "\n",
    "{\n",
    "    '1' : {\n",
    "        'mean': 4000,\n",
    "        'std: 2000,\n",
    "        'count': 34\n",
    "    },\n",
    "    '2' : {\n",
    "        'mean': 4300,\n",
    "        'std: 2300,\n",
    "        'count': 22\n",
    "    },    \n",
    "}\n",
    "\n",
    "\n",
    "__Now we can remove those 2 BHK apartments whose price_per_sqft is less than mean price_per_sqft of 1 BHK apartment__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd7e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bhk_outliers(df):\n",
    "    exclude_indices=np.array([])\n",
    "    for location, location_df in df.groupby ('location'):\n",
    "        bhk_stats = {}\n",
    "        for bhk, bhk_df in location_df.groupby('bhk'):\n",
    "             bhk_stats[bhk]={\n",
    "                  'mean': np.mean (bhk_df.price_per_sqft),\n",
    "                  'std': np.std (bhk_df.price_per_sqft),\n",
    "                  'count': bhk_df.shape[0]\n",
    "             }\n",
    "        for bhk, bhk_df in location_df.groupby('bhk'):\n",
    "            stats = bhk_stats.get(bhk-1)\n",
    "            if stats and stats['count']>5:\n",
    "                 exclude_indices=np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft<(stats['mean'])].index.values)\n",
    "    return df.drop(exclude_indices, axis='index')\n",
    "                                                                   \n",
    "data7=remove_bhk_outliers(data6)\n",
    "data7. shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce0c878",
   "metadata": {},
   "source": [
    "__Plot same scatter chart again to visualize price_per_sqft for 2 BHK and 3 BHK properties__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b91d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_chart(data7, \"Hebbal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e22a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_chart(data7, \"Rajaji Nagar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae9c72c",
   "metadata": {},
   "source": [
    "__Based on above charts we can see that data points which are outliers are removed due to remove_bhk_outliers function__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b06161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.figsize\"]=(20, 10)\n",
    "plt.hist(data7.price_per_sqft,rwidth=0.8)\n",
    "plt.xlabel(\"Price Per Square Feet\")\n",
    "plt.ylabel(\"Count\")                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64965223",
   "metadata": {},
   "source": [
    "### __Outlier Removal Using Bathrooms Feature__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e24b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data7.bath.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8fb61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data7[data7.bath>10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb399dc",
   "metadata": {},
   "source": [
    "__It is unusual to have 2 more bathrooms than number of bedrooms in a home__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9fdff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data7[data7.bath>data7.bhk+2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba23b71a",
   "metadata": {},
   "source": [
    "__If you have 4 bedroom home and even if you have bathroom in all 4 rooms plus one guest bathroom, you will have total bath = total bed + 1 max. Anything above that is an outlier or a data error and can be removed__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data8 = data7[data7.bath<data7.bhk+2]\n",
    "data8.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b49015",
   "metadata": {},
   "source": [
    "__Price_per_sqft was used for outlier detection only, so can be dropped. Also size can be dropped as we have bhk.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b4731",
   "metadata": {},
   "outputs": [],
   "source": [
    "data9 = data8.drop(['size','price_per_sqft'],axis='columns')\n",
    "data9.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ef7fe",
   "metadata": {},
   "source": [
    "## __Use One Hot Encoding For Location__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db2cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(data9.location)\n",
    "dummies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba68d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data10 = pd.concat([data9,dummies.drop('other',axis='columns')],axis='columns')\n",
    "data10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca97911",
   "metadata": {},
   "outputs": [],
   "source": [
    "data11 = data10.drop('location',axis='columns')\n",
    "data11.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f8cb01",
   "metadata": {},
   "source": [
    "### __Build a Model Now...__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ca2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data11.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c05c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data11.drop(['price'],axis='columns')\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81903a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be586136",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data11.price\n",
    "y.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a4f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a70861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0b72e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr_clf = LinearRegression()\n",
    "lr_clf.fit(X_train,y_train)\n",
    "lr_clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec9c45e",
   "metadata": {},
   "source": [
    "### __Use K Fold cross validation to measure accuracy of our LinearRegression model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa176b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "\n",
    "cross_val_score(LinearRegression(), X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2daad",
   "metadata": {},
   "source": [
    "__We can see that in 5 iterations we get a score above 80% all the time. This is pretty good but we want to test few other algorithms for regression to see if we can get even better score. We will use GridSearchCV for this purpose__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80447df6",
   "metadata": {},
   "source": [
    "### __Find best model using GridSearchCV__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def find_best_model_using_gridsearchcv(X,y):\n",
    "    algos = {\n",
    "        'linear_regression' : {\n",
    "            'model': LinearRegression(),\n",
    "            'params': {\n",
    "                'normalize': [True, False]\n",
    "            }\n",
    "        },\n",
    "        'lasso': {\n",
    "            'model': Lasso(),\n",
    "            'params': {\n",
    "                'alpha': [1,2],\n",
    "                'selection': ['random', 'cyclic']\n",
    "            }\n",
    "        },\n",
    "        'decision_tree': {\n",
    "            'model': DecisionTreeRegressor(),\n",
    "            'params': {\n",
    "                'criterion' : ['mse','friedman_mse'],\n",
    "                'splitter': ['best','random']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    for algo_name, config in algos.items():\n",
    "        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)\n",
    "        gs.fit(X,y)\n",
    "        scores.append({\n",
    "            'model': algo_name,\n",
    "            'best_score': gs.best_score_,\n",
    "            'best_params': gs.best_params_\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "\n",
    "find_best_model_using_gridsearchcv(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec244132",
   "metadata": {},
   "source": [
    "__Based on above results we can say that LinearRegression gives the best score. Hence we will use that.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386e5813",
   "metadata": {},
   "source": [
    "### Test the model for few properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9695a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price(location,sqft,bath,bhk):    \n",
    "    loc_index = np.where(X.columns==location)[0][0]\n",
    "\n",
    "    x = np.zeros(len(X.columns))\n",
    "    x[0] = sqft\n",
    "    x[1] = bath\n",
    "    x[2] = bhk\n",
    "    if loc_index >= 0:\n",
    "        x[loc_index] = 1\n",
    "\n",
    "    return lr_clf.predict([x])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e34c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_price('1st Phase JP Nagar',1000, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a85c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_price('Indira Nagar',1000, 3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831a0578",
   "metadata": {},
   "source": [
    "### __Export the tested model to a pickle file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('home_prices_model.pickle','wb') as f:\n",
    "    pickle.dump(lr_clf,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66c032",
   "metadata": {},
   "source": [
    "### __Export location and column information to a file that will be useful later on in our prediction application__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c7bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "columns = {\n",
    "    'data_columns' : [col.lower() for col in X.columns]\n",
    "}\n",
    "with open(\"columns.json\",\"w\") as f:\n",
    "    f.write(json.dumps(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f859b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
